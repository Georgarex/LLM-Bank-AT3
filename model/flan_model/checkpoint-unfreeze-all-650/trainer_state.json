{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 650,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.15384615384615385,
      "grad_norm": 123.4156265258789,
      "learning_rate": 4.930769230769231e-05,
      "loss": 37.9639,
      "step": 10
    },
    {
      "epoch": 0.3076923076923077,
      "grad_norm": 112.96499633789062,
      "learning_rate": 4.853846153846154e-05,
      "loss": 28.1038,
      "step": 20
    },
    {
      "epoch": 0.46153846153846156,
      "grad_norm": 146.39199829101562,
      "learning_rate": 4.7769230769230774e-05,
      "loss": 21.5555,
      "step": 30
    },
    {
      "epoch": 0.6153846153846154,
      "grad_norm": 120.85073852539062,
      "learning_rate": 4.7e-05,
      "loss": 14.3311,
      "step": 40
    },
    {
      "epoch": 0.7692307692307693,
      "grad_norm": 60.91881561279297,
      "learning_rate": 4.6230769230769234e-05,
      "loss": 9.1201,
      "step": 50
    },
    {
      "epoch": 0.9230769230769231,
      "grad_norm": 27.153039932250977,
      "learning_rate": 4.5461538461538464e-05,
      "loss": 7.1671,
      "step": 60
    },
    {
      "epoch": 1.0769230769230769,
      "grad_norm": 23.46660041809082,
      "learning_rate": 4.4692307692307693e-05,
      "loss": 5.452,
      "step": 70
    },
    {
      "epoch": 1.2307692307692308,
      "grad_norm": 21.264102935791016,
      "learning_rate": 4.392307692307693e-05,
      "loss": 5.1712,
      "step": 80
    },
    {
      "epoch": 1.3846153846153846,
      "grad_norm": 22.452449798583984,
      "learning_rate": 4.315384615384615e-05,
      "loss": 4.4526,
      "step": 90
    },
    {
      "epoch": 1.5384615384615383,
      "grad_norm": 21.83493423461914,
      "learning_rate": 4.238461538461539e-05,
      "loss": 4.1864,
      "step": 100
    },
    {
      "epoch": 1.6923076923076923,
      "grad_norm": 22.845035552978516,
      "learning_rate": 4.161538461538462e-05,
      "loss": 3.9328,
      "step": 110
    },
    {
      "epoch": 1.8461538461538463,
      "grad_norm": 24.528446197509766,
      "learning_rate": 4.084615384615385e-05,
      "loss": 3.7414,
      "step": 120
    },
    {
      "epoch": 2.0,
      "grad_norm": 25.53569221496582,
      "learning_rate": 4.007692307692308e-05,
      "loss": 3.5339,
      "step": 130
    },
    {
      "epoch": 2.1538461538461537,
      "grad_norm": 25.71190071105957,
      "learning_rate": 3.930769230769231e-05,
      "loss": 3.3212,
      "step": 140
    },
    {
      "epoch": 2.3076923076923075,
      "grad_norm": 29.533662796020508,
      "learning_rate": 3.853846153846154e-05,
      "loss": 3.1771,
      "step": 150
    },
    {
      "epoch": 2.4615384615384617,
      "grad_norm": 27.480993270874023,
      "learning_rate": 3.7769230769230775e-05,
      "loss": 2.9584,
      "step": 160
    },
    {
      "epoch": 2.6153846153846154,
      "grad_norm": 27.696611404418945,
      "learning_rate": 3.7e-05,
      "loss": 2.7553,
      "step": 170
    },
    {
      "epoch": 2.769230769230769,
      "grad_norm": 28.774974822998047,
      "learning_rate": 3.6230769230769235e-05,
      "loss": 2.5439,
      "step": 180
    },
    {
      "epoch": 2.9230769230769234,
      "grad_norm": 31.495433807373047,
      "learning_rate": 3.5461538461538464e-05,
      "loss": 2.3528,
      "step": 190
    },
    {
      "epoch": 3.076923076923077,
      "grad_norm": 28.145402908325195,
      "learning_rate": 3.4692307692307694e-05,
      "loss": 2.236,
      "step": 200
    },
    {
      "epoch": 3.230769230769231,
      "grad_norm": 29.56219482421875,
      "learning_rate": 3.3923076923076924e-05,
      "loss": 2.0631,
      "step": 210
    },
    {
      "epoch": 3.3846153846153846,
      "grad_norm": 29.17512321472168,
      "learning_rate": 3.315384615384616e-05,
      "loss": 1.9784,
      "step": 220
    },
    {
      "epoch": 3.5384615384615383,
      "grad_norm": 29.92902946472168,
      "learning_rate": 3.2384615384615384e-05,
      "loss": 1.8165,
      "step": 230
    },
    {
      "epoch": 3.6923076923076925,
      "grad_norm": 27.375368118286133,
      "learning_rate": 3.161538461538462e-05,
      "loss": 1.6803,
      "step": 240
    },
    {
      "epoch": 3.8461538461538463,
      "grad_norm": 27.010150909423828,
      "learning_rate": 3.084615384615385e-05,
      "loss": 1.5697,
      "step": 250
    },
    {
      "epoch": 4.0,
      "grad_norm": 27.328947067260742,
      "learning_rate": 3.007692307692308e-05,
      "loss": 1.4187,
      "step": 260
    },
    {
      "epoch": 4.153846153846154,
      "grad_norm": 27.356420516967773,
      "learning_rate": 2.930769230769231e-05,
      "loss": 1.3526,
      "step": 270
    },
    {
      "epoch": 4.3076923076923075,
      "grad_norm": 24.79388427734375,
      "learning_rate": 2.8538461538461543e-05,
      "loss": 1.2837,
      "step": 280
    },
    {
      "epoch": 4.461538461538462,
      "grad_norm": 23.063217163085938,
      "learning_rate": 2.776923076923077e-05,
      "loss": 1.2162,
      "step": 290
    },
    {
      "epoch": 4.615384615384615,
      "grad_norm": 22.353628158569336,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 1.0944,
      "step": 300
    },
    {
      "epoch": 4.769230769230769,
      "grad_norm": 21.182205200195312,
      "learning_rate": 2.6230769230769232e-05,
      "loss": 1.0285,
      "step": 310
    },
    {
      "epoch": 4.923076923076923,
      "grad_norm": 19.93328285217285,
      "learning_rate": 2.5461538461538465e-05,
      "loss": 1.0005,
      "step": 320
    },
    {
      "epoch": 5.076923076923077,
      "grad_norm": 18.818721771240234,
      "learning_rate": 2.4692307692307692e-05,
      "loss": 0.9286,
      "step": 330
    },
    {
      "epoch": 5.230769230769231,
      "grad_norm": 17.92821502685547,
      "learning_rate": 2.392307692307692e-05,
      "loss": 0.855,
      "step": 340
    },
    {
      "epoch": 5.384615384615385,
      "grad_norm": 17.545385360717773,
      "learning_rate": 2.3153846153846155e-05,
      "loss": 0.8317,
      "step": 350
    },
    {
      "epoch": 5.538461538461538,
      "grad_norm": 16.956722259521484,
      "learning_rate": 2.2384615384615385e-05,
      "loss": 0.7837,
      "step": 360
    },
    {
      "epoch": 5.6923076923076925,
      "grad_norm": 15.582083702087402,
      "learning_rate": 2.1615384615384614e-05,
      "loss": 0.7259,
      "step": 370
    },
    {
      "epoch": 5.846153846153846,
      "grad_norm": 14.8266019821167,
      "learning_rate": 2.0846153846153844e-05,
      "loss": 0.7025,
      "step": 380
    },
    {
      "epoch": 6.0,
      "grad_norm": 13.348971366882324,
      "learning_rate": 2.0076923076923077e-05,
      "loss": 0.6483,
      "step": 390
    },
    {
      "epoch": 6.153846153846154,
      "grad_norm": 12.363584518432617,
      "learning_rate": 1.930769230769231e-05,
      "loss": 0.6503,
      "step": 400
    },
    {
      "epoch": 6.3076923076923075,
      "grad_norm": 12.12060832977295,
      "learning_rate": 1.853846153846154e-05,
      "loss": 0.6139,
      "step": 410
    },
    {
      "epoch": 6.461538461538462,
      "grad_norm": 10.637632369995117,
      "learning_rate": 1.776923076923077e-05,
      "loss": 0.587,
      "step": 420
    },
    {
      "epoch": 6.615384615384615,
      "grad_norm": 10.126452445983887,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.53,
      "step": 430
    },
    {
      "epoch": 6.769230769230769,
      "grad_norm": 9.888896942138672,
      "learning_rate": 1.6230769230769233e-05,
      "loss": 0.5501,
      "step": 440
    },
    {
      "epoch": 6.923076923076923,
      "grad_norm": 8.938054084777832,
      "learning_rate": 1.5461538461538463e-05,
      "loss": 0.5041,
      "step": 450
    },
    {
      "epoch": 7.076923076923077,
      "grad_norm": 9.122647285461426,
      "learning_rate": 1.4692307692307694e-05,
      "loss": 0.4997,
      "step": 460
    },
    {
      "epoch": 7.230769230769231,
      "grad_norm": 8.156795501708984,
      "learning_rate": 1.3923076923076924e-05,
      "loss": 0.4741,
      "step": 470
    },
    {
      "epoch": 7.384615384615385,
      "grad_norm": 8.195794105529785,
      "learning_rate": 1.3153846153846156e-05,
      "loss": 0.4632,
      "step": 480
    },
    {
      "epoch": 7.538461538461538,
      "grad_norm": 8.811878204345703,
      "learning_rate": 1.2384615384615385e-05,
      "loss": 0.4798,
      "step": 490
    },
    {
      "epoch": 7.6923076923076925,
      "grad_norm": 7.361545085906982,
      "learning_rate": 1.1615384615384615e-05,
      "loss": 0.463,
      "step": 500
    },
    {
      "epoch": 7.846153846153846,
      "grad_norm": 7.304485321044922,
      "learning_rate": 1.0846153846153847e-05,
      "loss": 0.4194,
      "step": 510
    },
    {
      "epoch": 8.0,
      "grad_norm": 7.168875694274902,
      "learning_rate": 1.0076923076923076e-05,
      "loss": 0.4296,
      "step": 520
    },
    {
      "epoch": 8.153846153846153,
      "grad_norm": 6.399986743927002,
      "learning_rate": 9.307692307692308e-06,
      "loss": 0.3959,
      "step": 530
    },
    {
      "epoch": 8.307692307692308,
      "grad_norm": 8.529491424560547,
      "learning_rate": 8.538461538461538e-06,
      "loss": 0.4477,
      "step": 540
    },
    {
      "epoch": 8.461538461538462,
      "grad_norm": 5.922311782836914,
      "learning_rate": 7.76923076923077e-06,
      "loss": 0.4021,
      "step": 550
    },
    {
      "epoch": 8.615384615384615,
      "grad_norm": 6.084481239318848,
      "learning_rate": 7.000000000000001e-06,
      "loss": 0.4016,
      "step": 560
    },
    {
      "epoch": 8.76923076923077,
      "grad_norm": 6.538386821746826,
      "learning_rate": 6.230769230769231e-06,
      "loss": 0.3868,
      "step": 570
    },
    {
      "epoch": 8.923076923076923,
      "grad_norm": 6.098548889160156,
      "learning_rate": 5.461538461538462e-06,
      "loss": 0.3779,
      "step": 580
    },
    {
      "epoch": 9.076923076923077,
      "grad_norm": 5.877716064453125,
      "learning_rate": 4.692307692307693e-06,
      "loss": 0.399,
      "step": 590
    },
    {
      "epoch": 9.23076923076923,
      "grad_norm": 6.011985778808594,
      "learning_rate": 3.923076923076923e-06,
      "loss": 0.3687,
      "step": 600
    },
    {
      "epoch": 9.384615384615385,
      "grad_norm": 5.726669788360596,
      "learning_rate": 3.153846153846154e-06,
      "loss": 0.3751,
      "step": 610
    },
    {
      "epoch": 9.538461538461538,
      "grad_norm": 5.592010021209717,
      "learning_rate": 2.3846153846153846e-06,
      "loss": 0.3701,
      "step": 620
    },
    {
      "epoch": 9.692307692307692,
      "grad_norm": 5.282076358795166,
      "learning_rate": 1.6153846153846154e-06,
      "loss": 0.3602,
      "step": 630
    },
    {
      "epoch": 9.846153846153847,
      "grad_norm": 5.819834232330322,
      "learning_rate": 8.461538461538462e-07,
      "loss": 0.3691,
      "step": 640
    },
    {
      "epoch": 10.0,
      "grad_norm": 5.325556755065918,
      "learning_rate": 7.692307692307692e-08,
      "loss": 0.3761,
      "step": 650
    }
  ],
  "logging_steps": 10,
  "max_steps": 650,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 120828828057600.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
